{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programming\\python files(not package)\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Device Check ---\n",
      "Using device: cpu\n",
      "--------------------\n",
      "\n",
      "--- Loading IAM Dataset ---\n",
      "Dataset loaded successfully.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 6482\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 976\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 2915\n",
      "    })\n",
      "})\n",
      "--------------------------\n",
      "\n",
      "--- Preprocessing Data ---\n",
      "Vocabulary Size: 79\n",
      "Characters:  !\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "--- Initializing DataLoaders ---\n",
      "Preprocessing complete. PyTorch DataLoaders created.\n",
      "--------------------------------\n",
      "\n",
      "--- Building CRNN Model ---\n",
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (map_to_seq): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (rnn): LSTM(64, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=80, bias=True)\n",
      ")\n",
      "--------------------------\n",
      "\n",
      "--- Training Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:39<00:00,  1.08s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 - Train Loss: 3.4502, Val Loss: 3.2469\n",
      "Model saved after epoch 1 to epoch_models\\handwriting_recognizer_epoch_1.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:49<00:00,  1.13s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:19<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 - Train Loss: 3.2204, Val Loss: 3.2303\n",
      "Model saved after epoch 2 to epoch_models\\handwriting_recognizer_epoch_2.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:46<00:00,  1.11s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 - Train Loss: 3.1430, Val Loss: 3.0423\n",
      "Model saved after epoch 3 to epoch_models\\handwriting_recognizer_epoch_3.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:32<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 - Train Loss: 2.7159, Val Loss: 2.4663\n",
      "Model saved after epoch 4 to epoch_models\\handwriting_recognizer_epoch_4.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:31<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 - Train Loss: 2.3234, Val Loss: 2.0884\n",
      "Model saved after epoch 5 to epoch_models\\handwriting_recognizer_epoch_5.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:45<00:00,  1.11s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 - Train Loss: 2.0133, Val Loss: 1.7647\n",
      "Model saved after epoch 6 to epoch_models\\handwriting_recognizer_epoch_6.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:31<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 - Train Loss: 1.7234, Val Loss: 1.4815\n",
      "Model saved after epoch 7 to epoch_models\\handwriting_recognizer_epoch_7.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:29<00:00,  1.03s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 - Train Loss: 1.4818, Val Loss: 1.2711\n",
      "Model saved after epoch 8 to epoch_models\\handwriting_recognizer_epoch_8.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:43<00:00,  1.10s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 - Train Loss: 1.2938, Val Loss: 1.1134\n",
      "Model saved after epoch 9 to epoch_models\\handwriting_recognizer_epoch_9.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:40<00:00,  1.09s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 - Train Loss: 1.1491, Val Loss: 0.9855\n",
      "Model saved after epoch 10 to epoch_models\\handwriting_recognizer_epoch_10.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:32<00:00,  1.05s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 - Train Loss: 1.0355, Val Loss: 0.9016\n",
      "Model saved after epoch 11 to epoch_models\\handwriting_recognizer_epoch_11.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:46<00:00,  1.12s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:15<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 - Train Loss: 0.9411, Val Loss: 0.8325\n",
      "Model saved after epoch 12 to epoch_models\\handwriting_recognizer_epoch_12.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:38<00:00,  1.08s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 - Train Loss: 0.8658, Val Loss: 0.7702\n",
      "Model saved after epoch 13 to epoch_models\\handwriting_recognizer_epoch_13.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:31<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 - Train Loss: 0.7967, Val Loss: 0.7331\n",
      "Model saved after epoch 14 to epoch_models\\handwriting_recognizer_epoch_14.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:34<00:00,  1.06s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 - Train Loss: 0.7412, Val Loss: 0.6864\n",
      "Model saved after epoch 15 to epoch_models\\handwriting_recognizer_epoch_15.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:33<00:00,  1.05s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 - Train Loss: 0.6955, Val Loss: 0.6741\n",
      "Model saved after epoch 16 to epoch_models\\handwriting_recognizer_epoch_16.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:32<00:00,  1.05s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 - Train Loss: 0.6519, Val Loss: 0.6258\n",
      "Model saved after epoch 17 to epoch_models\\handwriting_recognizer_epoch_17.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:31<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 - Train Loss: 0.6156, Val Loss: 0.6039\n",
      "Model saved after epoch 18 to epoch_models\\handwriting_recognizer_epoch_18.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:30<00:00,  1.03s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 - Train Loss: 0.5830, Val Loss: 0.5762\n",
      "Model saved after epoch 19 to epoch_models\\handwriting_recognizer_epoch_19.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:40<00:00,  1.09s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 - Train Loss: 0.5504, Val Loss: 0.5686\n",
      "Model saved after epoch 20 to epoch_models\\handwriting_recognizer_epoch_20.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:37<00:00,  1.07s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 - Train Loss: 0.5301, Val Loss: 0.5481\n",
      "Model saved after epoch 21 to epoch_models\\handwriting_recognizer_epoch_21.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:33<00:00,  1.05s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 - Train Loss: 0.4978, Val Loss: 0.5510\n",
      "Model saved after epoch 22 to epoch_models\\handwriting_recognizer_epoch_22.pth\n",
      "No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:50<00:00,  1.14s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 - Train Loss: 0.4784, Val Loss: 0.5463\n",
      "Model saved after epoch 23 to epoch_models\\handwriting_recognizer_epoch_23.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:51<00:00,  1.14s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:13<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 - Train Loss: 0.4599, Val Loss: 0.5416\n",
      "Model saved after epoch 24 to epoch_models\\handwriting_recognizer_epoch_24.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:31<00:00,  1.04s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 - Train Loss: 0.4433, Val Loss: 0.5124\n",
      "Model saved after epoch 25 to epoch_models\\handwriting_recognizer_epoch_25.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:27<00:00,  1.02s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 - Train Loss: 0.4170, Val Loss: 0.5346\n",
      "Model saved after epoch 26 to epoch_models\\handwriting_recognizer_epoch_26.pth\n",
      "No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:22<00:00,  1.00it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 - Train Loss: 0.4000, Val Loss: 0.5103\n",
      "Model saved after epoch 27 to epoch_models\\handwriting_recognizer_epoch_27.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:17<00:00,  1.03it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 - Train Loss: 0.3856, Val Loss: 0.5042\n",
      "Model saved after epoch 28 to epoch_models\\handwriting_recognizer_epoch_28.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:19<00:00,  1.02it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 - Train Loss: 0.3668, Val Loss: 0.5092\n",
      "Model saved after epoch 29 to epoch_models\\handwriting_recognizer_epoch_29.pth\n",
      "No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:23<00:00,  1.00s/it]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 - Train Loss: 0.3581, Val Loss: 0.5100\n",
      "Model saved after epoch 30 to epoch_models\\handwriting_recognizer_epoch_30.pth\n",
      "No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:20<00:00,  1.01it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 - Train Loss: 0.3457, Val Loss: 0.5039\n",
      "Model saved after epoch 31 to epoch_models\\handwriting_recognizer_epoch_31.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:19<00:00,  1.02it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 - Train Loss: 0.3310, Val Loss: 0.4941\n",
      "Model saved after epoch 32 to epoch_models\\handwriting_recognizer_epoch_32.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:19<00:00,  1.02it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 - Train Loss: 0.3148, Val Loss: 0.4979\n",
      "Model saved after epoch 33 to epoch_models\\handwriting_recognizer_epoch_33.pth\n",
      "No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:21<00:00,  1.01it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 - Train Loss: 0.3039, Val Loss: 0.5121\n",
      "Model saved after epoch 34 to epoch_models\\handwriting_recognizer_epoch_34.pth\n",
      "No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:19<00:00,  1.02it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 - Train Loss: 0.2977, Val Loss: 0.5043\n",
      "Model saved after epoch 35 to epoch_models\\handwriting_recognizer_epoch_35.pth\n",
      "No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:19<00:00,  1.02it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 - Train Loss: 0.2825, Val Loss: 0.4941\n",
      "Model saved after epoch 36 to epoch_models\\handwriting_recognizer_epoch_36.pth\n",
      "Model improved. Saved best model to handwriting_recognizer_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:21<00:00,  1.01it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 - Train Loss: 0.2792, Val Loss: 0.5058\n",
      "Model saved after epoch 37 to epoch_models\\handwriting_recognizer_epoch_37.pth\n",
      "No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [03:20<00:00,  1.01it/s]\n",
      "Validating: 100%|██████████| 31/31 [00:12<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 - Train Loss: 0.2662, Val Loss: 0.5179\n",
      "Model saved after epoch 38 to epoch_models\\handwriting_recognizer_epoch_38.pth\n",
      "No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 44/203 [00:49<02:57,  1.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 242\u001b[39m\n\u001b[32m    239\u001b[39m     os.makedirs(epoch_models_dir)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     val_loss = validate_one_epoch(model, val_loader, criterion, device)\n\u001b[32m    245\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device)\u001b[39m\n\u001b[32m    169\u001b[39m input_lengths = torch.full(size=(images.size(\u001b[32m0\u001b[39m),), fill_value=log_probs.size(\u001b[32m0\u001b[39m), dtype=torch.long)\n\u001b[32m    170\u001b[39m loss = criterion(log_probs, labels, input_lengths, label_lengths)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m optimizer.step()\n\u001b[32m    173\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\programming\\python files(not package)\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\programming\\python files(not package)\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\programming\\python files(not package)\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Handwritten Text Recognition with a CRNN Model (PyTorch).\n",
    "\n",
    "This notebook implements a complete pipeline for training and evaluating a\n",
    "Convolutional Recurrent Neural Network (CRNN) for handwritten text recognition\n",
    "on the IAM dataset using PyTorch.\n",
    "\n",
    "This version includes fixes for the common DataLoader error on Windows and\n",
    "the KeyError for 'image_path' by using the correct 'image' field from the dataset.\n",
    "\n",
    "Changes:\n",
    "1.  The main execution block is wrapped in `if __name__ == '__main__':`.\n",
    "2.  `num_workers` is set to 0 in the DataLoader for Windows compatibility.\n",
    "3.  Fixed `KeyError` by using `item['image']` instead of `item['image_path']`.\n",
    "4.  Improved error handling in Dataset and collate_fn to prevent recursion errors.\n",
    "\"\"\"\n",
    "\n",
    "# 1. SETUP AND IMPORTS\n",
    "# ==============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress verbose dataset loading logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# 2. DEVICE CONFIGURATION\n",
    "# ==============================================================================\n",
    "print(\"--- Device Check ---\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "\n",
    "# 3. DATASET LOADING AND PREPARATION\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Loading IAM Dataset ---\")\n",
    "try:\n",
    "    iam_dataset = load_dataset(\"Teklia/IAM-line\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(iam_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset. Please check your internet connection. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Split the dataset\n",
    "train_hf_dataset = iam_dataset[\"train\"]\n",
    "val_hf_dataset = iam_dataset[\"validation\"]\n",
    "test_hf_dataset = iam_dataset[\"test\"]\n",
    "print(\"--------------------------\")\n",
    "\n",
    "\n",
    "# 4. PREPROCESSING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Preprocessing Data ---\")\n",
    "\n",
    "# --- Text Preprocessing ---\n",
    "characters = set()\n",
    "for item in train_hf_dataset:\n",
    "    characters.update(list(item['text']))\n",
    "characters = sorted(list(characters))\n",
    "VOCAB = \"\".join(characters)\n",
    "\n",
    "char_to_int = {char: i + 1 for i, char in enumerate(VOCAB)} # 0 is reserved for blank\n",
    "int_to_char = {i + 1: char for i, char in enumerate(VOCAB)}\n",
    "CTC_BLANK = 0\n",
    "\n",
    "print(f\"Vocabulary Size: {len(VOCAB)}\")\n",
    "print(f\"Characters: {VOCAB}\")\n",
    "\n",
    "# --- Image Preprocessing ---\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 512\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# --- Custom PyTorch Dataset ---\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.hf_dataset[idx]\n",
    "            image = item['image'].convert(\"RGB\")\n",
    "            text = item['text']\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            label = [char_to_int[char] for char in text]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing item at index {idx}. Error: {e}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "# --- Collate Function for DataLoader ---\n",
    "def collate_fn(batch):\n",
    "    # FIX: Filter out None values that may be returned by a failing __getitem__\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch:\n",
    "        # Return empty tensors if the whole batch failed\n",
    "        return torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "    padded_labels = nn.utils.rnn.pad_sequence(list(labels), batch_first=True, padding_value=0)\n",
    "    return images, padded_labels, label_lengths\n",
    "\n",
    "\n",
    "# 5. MODEL BUILDING (CRNN)\n",
    "# ==============================================================================\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.map_to_seq = nn.Linear(64 * (IMG_HEIGHT // 4), 64)\n",
    "        self.rnn = nn.LSTM(64, 128, num_layers=2, bidirectional=True, dropout=0.25)\n",
    "        self.fc = nn.Linear(256, num_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        b, w, c, h = x.size()\n",
    "        x = x.view(b, w, c * h)\n",
    "        x = self.map_to_seq(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.log_softmax(x, dim=2)\n",
    "        return x\n",
    "\n",
    "# 6. TRAINING AND VALIDATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, labels, label_lengths in tqdm(dataloader, desc=\"Training\"):\n",
    "        # FIX: Handle case where a whole batch might be empty\n",
    "        if images.size(0) == 0:\n",
    "            continue\n",
    "        images, labels, label_lengths = images.to(device), labels.to(device), label_lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(images)\n",
    "        input_lengths = torch.full(size=(images.size(0),), fill_value=log_probs.size(0), dtype=torch.long)\n",
    "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, label_lengths in tqdm(dataloader, desc=\"Validating\"):\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            images, labels, label_lengths = images.to(device), labels.to(device), label_lengths.to(device)\n",
    "            log_probs = model(images)\n",
    "            input_lengths = torch.full(size=(images.size(0),), fill_value=log_probs.size(0), dtype=torch.long)\n",
    "            loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 7. INFERENCE FUNCTION\n",
    "# ==============================================================================\n",
    "def ctc_decode(log_probs):\n",
    "    preds = log_probs.argmax(dim=2).permute(1, 0)\n",
    "    decoded_texts = []\n",
    "    for pred in preds:\n",
    "        s = ''.join([int_to_char.get(c.item(), '') for c in pred if c != CTC_BLANK])\n",
    "        dedup_s = \"\"\n",
    "        if s:\n",
    "            dedup_s = s[0]\n",
    "            for char in s[1:]:\n",
    "                if char != dedup_s[-1]:\n",
    "                    dedup_s += char\n",
    "        decoded_texts.append(dedup_s)\n",
    "    return decoded_texts\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Initializing DataLoaders ---\")\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    train_dataset = IAMDataset(train_hf_dataset, transform=transform)\n",
    "    val_dataset = IAMDataset(val_hf_dataset, transform=transform)\n",
    "\n",
    "    # **FIX**: Set num_workers=0 for Windows compatibility\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(\"Preprocessing complete. PyTorch DataLoaders created.\")\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    print(\"\\n--- Building CRNN Model ---\")\n",
    "    model = CRNN(num_chars=len(VOCAB) + 1).to(device)\n",
    "    print(model)\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "    print(\"\\n--- Training Model ---\")\n",
    "    criterion = nn.CTCLoss(blank=CTC_BLANK, zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    epoch_models_dir = 'epoch_models'\n",
    "    if not os.path.exists(epoch_models_dir):\n",
    "        os.makedirs(epoch_models_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        epoch_save_path = os.path.join(epoch_models_dir, f'handwriting_recognizer_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), epoch_save_path)\n",
    "        print(f\"Model saved after epoch {epoch+1} to {epoch_save_path}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'handwriting_recognizer_best.pth')\n",
    "            print(f\"Model improved. Saved best model to handwriting_recognizer_best.pth\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    print(\"\\n--- Evaluating Model and Running Inference ---\")\n",
    "    prediction_model = CRNN(num_chars=len(VOCAB) + 1).to(device)\n",
    "    prediction_model.load_state_dict(torch.load('handwriting_recognizer_best.pth', map_location=device))\n",
    "    prediction_model.eval()\n",
    "\n",
    "    data_iter = iter(val_loader)\n",
    "    images, labels, _ = next(data_iter)\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_probs = prediction_model(images)\n",
    "\n",
    "    pred_texts = ctc_decode(log_probs)\n",
    "\n",
    "    orig_texts = []\n",
    "    for label_tensor in labels:\n",
    "        text = \"\".join([int_to_char.get(c.item(), '') for c in label_tensor if c != 0])\n",
    "        orig_texts.append(text)\n",
    "\n",
    "    _, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "\n",
    "    for i in range(min(16, BATCH_SIZE)):\n",
    "        if i >= images.size(0):\n",
    "            break\n",
    "        img = images[i].cpu().numpy().squeeze()\n",
    "        img = (img * 0.5 + 0.5) * 255\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        ax = axes[i // 4, i % 4]\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(f\"True: {orig_texts[i]}\\nPred: {pred_texts[i]}\", fontsize=9)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Model Predictions on Validation Set (PyTorch)\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "    print(\"\\n--- Saving Final Model Locally ---\")\n",
    "    torch.save(model.state_dict(), \"handwriting_recognizer_final.pth\")\n",
    "    print(\"Final model state dict saved as 'handwriting_recognizer_final.pth'\")\n",
    "    print(\"Best performing model state dict saved as 'handwriting_recognizer_best.pth'\")\n",
    "    print(f\"All epoch-wise models are saved in the '{epoch_models_dir}/' directory.\")\n",
    "    print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model and Running Inference ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Evaluating Model and Running Inference ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m prediction_model = \u001b[43mCRNN\u001b[49m(num_chars=\u001b[38;5;28mlen\u001b[39m(VOCAB) + \u001b[32m1\u001b[39m).to(device)\n\u001b[32m      3\u001b[39m prediction_model.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33mhandwriting_recognizer_best.pth\u001b[39m\u001b[33m'\u001b[39m, map_location=device))\n\u001b[32m      4\u001b[39m prediction_model.eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'CRNN' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluating Model and Running Inference ---\")\n",
    "prediction_model = CRNN(num_chars=len(VOCAB) + 1).to(device)\n",
    "prediction_model.load_state_dict(torch.load('handwriting_recognizer_best.pth', map_location=device))\n",
    "prediction_model.eval()\n",
    "\n",
    "data_iter = iter(val_loader)\n",
    "images, labels, _ = next(data_iter)\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_probs = prediction_model(images)\n",
    "\n",
    "pred_texts = ctc_decode(log_probs)\n",
    "\n",
    "orig_texts = []\n",
    "for label_tensor in labels:\n",
    "    text = \"\".join([int_to_char.get(c.item(), '') for c in label_tensor if c != 0])\n",
    "    orig_texts.append(text)\n",
    "\n",
    "_, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "\n",
    "for i in range(min(16, BATCH_SIZE)):\n",
    "    if i >= images.size(0):\n",
    "        break\n",
    "    img = images[i].cpu().numpy().squeeze()\n",
    "    img = (img * 0.5 + 0.5) * 255\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(f\"True: {orig_texts[i]}\\nPred: {pred_texts[i]}\", fontsize=9)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Model Predictions on Validation Set (PyTorch)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "print(\"\\n--- Saving Final Model Locally ---\")\n",
    "torch.save(model.state_dict(), \"handwriting_recognizer_final.pth\")\n",
    "print(\"Final model state dict saved as 'handwriting_recognizer_final.pth'\")\n",
    "print(\"Best performing model state dict saved as 'handwriting_recognizer_best.pth'\")\n",
    "print(f\"All epoch-wise models are saved in the '{epoch_models_dir}/' directory.\")\n",
    "print(\"----------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
