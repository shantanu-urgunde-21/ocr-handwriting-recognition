{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP AND IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress verbose dataset loading logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# 2. DEVICE CONFIGURATION\n",
    "print(\"--- Device Check ---\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "\n",
    "# 3. DATASET LOADING AND PREPARATION\n",
    "print(\"\\n--- Loading IAM Dataset ---\")\n",
    "try:\n",
    "    iam_dataset = load_dataset(\"Teklia/IAM-line\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(iam_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset. Please check your internet connection. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Split the dataset\n",
    "train_hf_dataset = iam_dataset[\"train\"]\n",
    "val_hf_dataset = iam_dataset[\"validation\"]\n",
    "test_hf_dataset = iam_dataset[\"test\"]\n",
    "print(\"--------------------------\")\n",
    "\n",
    "\n",
    "# 4. PREPROCESSING\n",
    "print(\"\\n--- Preprocessing Data ---\")\n",
    "\n",
    "# --- Text Preprocessing ---\n",
    "characters = set()\n",
    "for item in train_hf_dataset:\n",
    "    characters.update(list(item['text']))\n",
    "characters = sorted(list(characters))\n",
    "VOCAB = \"\".join(characters)\n",
    "\n",
    "char_to_int = {char: i + 1 for i, char in enumerate(VOCAB)} # 0 is reserved for blank\n",
    "int_to_char = {i + 1: char for i, char in enumerate(VOCAB)}\n",
    "CTC_BLANK = 0\n",
    "\n",
    "print(f\"Vocabulary Size: {len(VOCAB)}\")\n",
    "print(f\"Characters: {VOCAB}\")\n",
    "\n",
    "# --- Image Preprocessing ---\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 512\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.hf_dataset[idx]\n",
    "            image = item['image'].convert(\"RGB\")\n",
    "            text = item['text']\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            label = [char_to_int[char] for char in text]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing item at index {idx}. Error: {e}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "# --- Collate Function for DataLoader ---\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch:\n",
    "        # Return empty tensors if the whole batch failed\n",
    "        return torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "    padded_labels = nn.utils.rnn.pad_sequence(list(labels), batch_first=True, padding_value=0)\n",
    "    return images, padded_labels, label_lengths\n",
    "\n",
    "\n",
    "# 5. MODEL BUILDING (CRNN)\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.map_to_seq = nn.Linear(64 * (IMG_HEIGHT // 4), 64)\n",
    "        self.rnn = nn.LSTM(64, 128, num_layers=2, bidirectional=True, dropout=0.25)\n",
    "        self.fc = nn.Linear(256, num_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        b, w, c, h = x.size()\n",
    "        x = x.view(b, w, c * h)\n",
    "        x = self.map_to_seq(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.log_softmax(x, dim=2)\n",
    "        return x\n",
    "\n",
    "# 6. TRAINING AND VALIDATION FUNCTIONS\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, labels, label_lengths in tqdm(dataloader, desc=\"Training\"):\n",
    "        # FIX: Handle case where a whole batch might be empty\n",
    "        if images.size(0) == 0:\n",
    "            continue\n",
    "        images, labels, label_lengths = images.to(device), labels.to(device), label_lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(images)\n",
    "        input_lengths = torch.full(size=(images.size(0),), fill_value=log_probs.size(0), dtype=torch.long)\n",
    "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, label_lengths in tqdm(dataloader, desc=\"Validating\"):\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            images, labels, label_lengths = images.to(device), labels.to(device), label_lengths.to(device)\n",
    "            log_probs = model(images)\n",
    "            input_lengths = torch.full(size=(images.size(0),), fill_value=log_probs.size(0), dtype=torch.long)\n",
    "            loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# 7. INFERENCE FUNCTION\n",
    "def ctc_decode(log_probs):\n",
    "    preds = log_probs.argmax(dim=2).permute(1, 0)\n",
    "    decoded_texts = []\n",
    "    for pred in preds:\n",
    "        s = ''.join([int_to_char.get(c.item(), '') for c in pred if c != CTC_BLANK])\n",
    "        dedup_s = \"\"\n",
    "        if s:\n",
    "            dedup_s = s[0]\n",
    "            for char in s[1:]:\n",
    "                if char != dedup_s[-1]:\n",
    "                    dedup_s += char\n",
    "        decoded_texts.append(dedup_s)\n",
    "    return decoded_texts\n",
    "\n",
    "# 8. main block\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Initializing DataLoaders ---\")\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    train_dataset = IAMDataset(train_hf_dataset, transform=transform)\n",
    "    val_dataset = IAMDataset(val_hf_dataset, transform=transform)\n",
    "\n",
    "    # **FIX**: Set num_workers=0 for Windows compatibility\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(\"Preprocessing complete. PyTorch DataLoaders created.\")\n",
    "\n",
    "    print(\"\\n--- Building CRNN Model ---\")\n",
    "    model = CRNN(num_chars=len(VOCAB) + 1).to(device)\n",
    "    print(model)\n",
    "\n",
    "    print(\"\\n--- Training Model ---\")\n",
    "    criterion = nn.CTCLoss(blank=CTC_BLANK, zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    epoch_models_dir = 'epoch_models'\n",
    "    if not os.path.exists(epoch_models_dir):\n",
    "        os.makedirs(epoch_models_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        epoch_save_path = os.path.join(epoch_models_dir, f'handwriting_recognizer_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), epoch_save_path)\n",
    "        print(f\"Model saved after epoch {epoch+1} to {epoch_save_path}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'handwriting_recognizer_best.pth')\n",
    "            print(f\"Model improved. Saved best model to handwriting_recognizer_best.pth\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    print(\"\\n--- Evaluating Model and Running Inference ---\")\n",
    "    prediction_model = CRNN(num_chars=len(VOCAB) + 1).to(device)\n",
    "    prediction_model.load_state_dict(torch.load('handwriting_recognizer_best.pth', map_location=device))\n",
    "    prediction_model.eval()\n",
    "\n",
    "    data_iter = iter(val_loader)\n",
    "    images, labels, _ = next(data_iter)\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_probs = prediction_model(images)\n",
    "\n",
    "    pred_texts = ctc_decode(log_probs)\n",
    "\n",
    "    orig_texts = []\n",
    "    for label_tensor in labels:\n",
    "        text = \"\".join([int_to_char.get(c.item(), '') for c in label_tensor if c != 0])\n",
    "        orig_texts.append(text)\n",
    "\n",
    "    _, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "\n",
    "    for i in range(min(16, BATCH_SIZE)):\n",
    "        if i >= images.size(0):\n",
    "            break\n",
    "        img = images[i].cpu().numpy().squeeze()\n",
    "        img = (img * 0.5 + 0.5) * 255\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        ax = axes[i // 4, i % 4]\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(f\"True: {orig_texts[i]}\\nPred: {pred_texts[i]}\", fontsize=9)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Model Predictions on Validation Set (PyTorch)\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"\\n--- Saving Final Model Locally ---\")\n",
    "    torch.save(model.state_dict(), \"handwriting_recognizer_final.pth\")\n",
    "    print(\"Final model state dict saved as 'handwriting_recognizer_final.pth'\")\n",
    "    print(\"Best performing model state dict saved as 'handwriting_recognizer_best.pth'\")\n",
    "    print(f\"All epoch-wise models are saved in the '{epoch_models_dir}/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating Model and Running Inference ---\")\n",
    "prediction_model = CRNN(num_chars=len(VOCAB) + 1).to(device)\n",
    "prediction_model.load_state_dict(torch.load('handwriting_recognizer_best.pth', map_location=device))\n",
    "prediction_model.eval()\n",
    "\n",
    "data_iter = iter(val_loader)\n",
    "images, labels, _ = next(data_iter)\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_probs = prediction_model(images)\n",
    "\n",
    "pred_texts = ctc_decode(log_probs)\n",
    "\n",
    "orig_texts = []\n",
    "for label_tensor in labels:\n",
    "    text = \"\".join([int_to_char.get(c.item(), '') for c in label_tensor if c != 0])\n",
    "    orig_texts.append(text)\n",
    "\n",
    "_, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "\n",
    "for i in range(min(16, BATCH_SIZE)):\n",
    "    if i >= images.size(0):\n",
    "        break\n",
    "    img = images[i].cpu().numpy().squeeze()\n",
    "    img = (img * 0.5 + 0.5) * 255\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(f\"True: {orig_texts[i]}\\nPred: {pred_texts[i]}\", fontsize=9)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Model Predictions on Validation Set (PyTorch)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n--- Saving Final Model Locally ---\")\n",
    "torch.save(model.state_dict(), \"handwriting_recognizer_final.pth\")\n",
    "print(\"Final model state dict saved as 'handwriting_recognizer_final.pth'\")\n",
    "print(\"Best performing model state dict saved as 'handwriting_recognizer_best.pth'\")\n",
    "print(f\"All epoch-wise models are saved in the '{epoch_models_dir}/' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
